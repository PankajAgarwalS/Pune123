{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnmcEv3UxbwW",
    "outputId": "1d74a500-4391-49b5-eb19-e638f7d6d3ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AGARWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AGARWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AGARWAL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TreebankWordTokenizer, WordPunctTokenizer, TweetTokenizer, MWETokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "shnz4FWmtng_"
   },
   "outputs": [],
   "source": [
    "text = \"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG50vGF0ERwJ"
   },
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7MOIu5DxpIE",
    "outputId": "4fb33c3a-5317-495a-9c9a-bb5b35f7a8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Tokenization:\n",
      "['N', 'L', 'T', 'K', ' ', 'i', 's', ' ', 'a', ' ', 'l', 'e', 'a', 'd', 'i', 'n', 'g', ' ', 'p', 'l', 'a', 't', 'f', 'o', 'r', 'm', ' ', 'f', 'o', 'r', ' ', 'b', 'u', 'i', 'l', 'd', 'i', 'n', 'g', ' ', 'P', 'y', 't', 'h', 'o', 'n', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 's', ' ', 't', 'o', ' ', 'w', 'o', 'r', 'k', ' ', 'w', 'i', 't', 'h', ' ', 'h', 'u', 'm', 'a', 'n', ' ', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e', ' ', 'd', 'a', 't', 'a', '.', ' ', 'I', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'e', 'a', 's', 'y', '-', 't', 'o', '-', 'u', 's', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'f', 'a', 'c', 'e', 's', ' ', 't', 'o', ' ', 'o', 'v', 'e', 'r', ' ', '5', '0', ' ', 'c', 'o', 'r', 'p', 'o', 'r', 'a', ' ', 'a', 'n', 'd', ' ', 'l', 'e', 'x', 'i', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'o', 'u', 'r', 'c', 'e', 's', '.']\n",
      "Length of characters list: 166\n"
     ]
    }
   ],
   "source": [
    "characters = list(text)\n",
    "print(\"Character Tokenization:\")\n",
    "print(characters)\n",
    "print(\"Length of characters list:\", len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLrw25pjxpul",
    "outputId": "92f4fa71-b023-4891-ba80-a29334f868a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
      "Length of words list: 28\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(\"\\nWord Tokenization:\")\n",
    "print(words)\n",
    "print(\"Length of words list:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvrsU-Mdxroc",
    "outputId": "55818f15-b414-426b-e1d4-ffbcafdfdcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokenization:\n",
      "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources.']\n",
      "Length of sentences list: 2\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(sentences)\n",
    "print(\"Length of sentences list:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CKk3cLl_ySh1",
    "outputId": "47cbe32e-bfcb-4742-db4d-be9d1de17658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TreebankWordTokenizer:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
      "Length of words list (TreebankWordTokenizer): 27\n"
     ]
    }
   ],
   "source": [
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "words_treebank = treebank_tokenizer.tokenize(text)\n",
    "print(\"\\nTreebankWordTokenizer:\")\n",
    "print(words_treebank)\n",
    "print(\"Length of words list (TreebankWordTokenizer):\", len(words_treebank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwSvYcQuyVLs",
    "outputId": "502f8400-c5e5-4f7b-adb3-5cc5e6cc38fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WordPunctTokenizer:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy', '-', 'to', '-', 'use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
      "Length of words list (WordPunctTokenizer): 32\n"
     ]
    }
   ],
   "source": [
    "wordpunct_tokenizer = WordPunctTokenizer()\n",
    "words_wordpunct = wordpunct_tokenizer.tokenize(text)\n",
    "print(\"\\nWordPunctTokenizer:\")\n",
    "print(words_wordpunct)\n",
    "print(\"Length of words list (WordPunctTokenizer):\", len(words_wordpunct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBfbtPjyyhpr",
    "outputId": "484968da-f7ad-423c-9ca5-066077b409fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TweetTokenizer:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
      "Length of words list (TweetTokenizer): 28\n"
     ]
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "words_tweet = tweet_tokenizer.tokenize(text)\n",
    "print(\"\\nTweetTokenizer:\")\n",
    "print(words_tweet)\n",
    "print(\"Length of words list (TweetTokenizer):\", len(words_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6_2_bdfyjqz",
    "outputId": "09bd534e-de60-46dd-b6ad-f51fc265dcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MWETokenizer:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources.']\n",
      "Length of words list (MWETokenizer): 26\n"
     ]
    }
   ],
   "source": [
    "mwe_tokenizer = MWETokenizer()\n",
    "words_mwe = mwe_tokenizer.tokenize(text.split())\n",
    "print(\"\\nMWETokenizer:\")\n",
    "print(words_mwe)\n",
    "print(\"Length of words list (MWETokenizer):\", len(words_mwe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYyHXS-SymRD",
    "outputId": "8688313a-b0fe-46d6-ee83-894188fa3b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob Word Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources']\n",
      "Length of words list (TextBlob): 26\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)\n",
    "words_blob = blob.words\n",
    "print(\"TextBlob Word Tokenization:\")\n",
    "print(words_blob)\n",
    "print(\"Length of words list (TextBlob):\", len(words_blob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7oh5yBsN0GQp",
    "outputId": "66e5f2f8-116f-424b-db92-6ab76e821a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy', '-', 'to', '-', 'use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', '.']\n",
      "Length of tokens list (spaCy): 32\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"spaCy Tokenization:\")\n",
    "print(tokens)\n",
    "print(\"Length of tokens list (spaCy):\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM0XVj-OEISG"
   },
   "source": [
    "### **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c5QfHfE0SD5",
    "outputId": "da03e8b8-74d6-477b-f441-610f986563e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word\t\tPorter Stemmed Word\n",
      "NLTK                \t\tnltk\n",
      "is                  \t\tis\n",
      "a                   \t\ta\n",
      "leading             \t\tlead\n",
      "platform            \t\tplatform\n",
      "for                 \t\tfor\n",
      "building            \t\tbuild\n",
      "Python              \t\tpython\n",
      "programs            \t\tprogram\n",
      "to                  \t\tto\n",
      "work                \t\twork\n",
      "with                \t\twith\n",
      "human               \t\thuman\n",
      "language            \t\tlanguag\n",
      "data                \t\tdata\n",
      ".                   \t\t.\n",
      "It                  \t\tit\n",
      "provides            \t\tprovid\n",
      "easy-to-use         \t\teasy-to-us\n",
      "interfaces          \t\tinterfac\n",
      "to                  \t\tto\n",
      "over                \t\tover\n",
      "50                  \t\t50\n",
      "corpora             \t\tcorpora\n",
      "and                 \t\tand\n",
      "lexical             \t\tlexic\n",
      "resources           \t\tresourc\n",
      ".                   \t\t.\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Original Word\\t\\tPorter Stemmed Word\")\n",
    "for word in words:\n",
    "    stemmed_word = porter_stemmer.stem(word)\n",
    "    print(f\"{word:20}\\t\\t{stemmed_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1r3MJvYY0vZS",
    "outputId": "86e1c64d-8d43-4556-9616-478b9c4e1a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming using SnowballStemmer:\n",
      "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'human', 'languag', 'data', '.', 'it', 'provid', 'easy-to-us', 'interfac', 'to', 'over', '50', 'corpora', 'and', 'lexic', 'resourc', '.']\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer(language='english')\n",
    "stemmed_snowball = [snowball_stemmer.stem(word) for word in words]\n",
    "print(\"\\nStemming using SnowballStemmer:\")\n",
    "print(stemmed_snowball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNKebVvI0xm5",
    "outputId": "fa26e3b4-ecd0-495f-9380-4bcd6b03f4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stemming using LancasterStemmer:\n",
      "['nltk', 'is', 'a', 'lead', 'platform', 'for', 'build', 'python', 'program', 'to', 'work', 'with', 'hum', 'langu', 'dat', '.', 'it', 'provid', 'easy-to-use', 'interfac', 'to', 'ov', '50', 'corpor', 'and', 'lex', 'resourc', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster_stemmer = LancasterStemmer()\n",
    "stemmed_lancaster = [lancaster_stemmer.stem(word) for word in words]\n",
    "print(\"\\nStemming using LancasterStemmer:\")\n",
    "print(stemmed_lancaster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTB3Swx8EMAc"
   },
   "source": [
    "### **Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FooLO2DD07DB",
    "outputId": "35c2ba60-a19d-4d42-dc0a-cc55a007a63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet Lemmatization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy', '-', 'to', '-', 'use', 'interface', 'to', 'over', '50', 'corpus', 'and', 'lexical', 'resource', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"WordNet Lemmatization:\")\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHyZhQjfFz53",
    "outputId": "f1356570-7867-4ff7-d73d-29a506540a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Lemmatization:\n",
      "['NLTK', 'be', 'a', 'lead', 'platform', 'for', 'build', 'Python', 'program', 'to', 'work', 'with', 'human', 'language', 'datum', '.', 'it', 'provide', 'easy', '-', 'to', '-', 'use', 'interface', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resource', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"spaCy Lemmatization:\")\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4iKYB-NGCRS"
   },
   "source": [
    "### **POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4edP2t_GFdi",
    "outputId": "146760c2-809f-4db3-f035-d5f0b41966a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('leading', 'VBG'), ('platform', 'NN'), ('for', 'IN'), ('building', 'VBG'), ('Python', 'NNP'), ('programs', 'NNS'), ('to', 'TO'), ('work', 'VB'), ('with', 'IN'), ('human', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('.', '.'), ('It', 'PRP'), ('provides', 'VBZ'), ('easy', 'JJ'), ('-', ':'), ('to', 'TO'), ('-', ':'), ('use', 'NN'), ('interfaces', 'NNS'), ('to', 'TO'), ('over', 'IN'), ('50', 'CD'), ('corpora', 'NNS'), ('and', 'CC'), ('lexical', 'JJ'), ('resources', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(tokens))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nM0XVj-OEISG"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
